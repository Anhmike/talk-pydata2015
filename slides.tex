\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{etex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{algpseudocode}
\usepackage{multirow}

% Code snippets
\usepackage{minted}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
\newminted{python}{bgcolor=bgcolor}

% Checked marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Colors
\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

% Beamer layout
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

% Front slide
\title{{\bf Tree models with Scikit-Learn}\\
Great learners with little assumptions}
\author{
Material: \url{https://github.com/glouppe/talk-pydata2015}\\
\vspace{1cm}
Gilles Louppe (\href{https://twitter.com/glouppe}{@glouppe})\\
{\it CERN}
}
\date{PyData, April 3, 2015}

% Argmax
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}


% Random things to talk about:
% - categorical features
% - sample weights
% - class_weights
% - oob samples
% outlier detection


% Outline =====================================================================

\begin{frame}
  \frametitle{Outline}
  %\tableofcontents
  \setbeamertemplate{enumerate items}[circle]
  \begin{enumerate}
  \item Motivation

  \vspace{0.5cm}

  \item Growing decision trees

  \vspace{0.5cm}

  \item Random forests

    \vspace{0.5cm}

  \item Boosting

  \vspace{0.5cm}

  \item Reading tree leaves

  \vspace{0.5cm}

  \item Summary
  \end{enumerate}
\end{frame}


% Motivation ==================================================================

\section{Motivation}

\begin{frame}
    \frametitle{Motivation}
    \begin{figure}
    \vspace{-0.5cm}
    \includegraphics[scale=0.4]{./figures/motivation.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Running example}

    \begin{columns}
    \begin{column}{0.5\textwidth}

    \begin{center}
    From {\bf \X{physicochemical properties}} (alcohol, acidity, sulphates, ...),

    \vspace{1cm}
    learn a {\bf \model{model}}
    \vspace{1cm}

    to predict {\bf \y{wine taste preferences}}.

    \end{center}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
      \vspace{-0.5cm}
      \includegraphics[scale=0.6]{./figures/wine.jpg}
      \end{figure}
    \end{column}
    \end{columns}
\end{frame}


% Growing decision trees ======================================================

\AtBeginSection[]
{
\begin{frame}[noframenumbering, plain]
  \frametitle{Outline}
  \tableofcontents[currentsection]
  % Die Option [pausesections]
\end{frame}
}

\section{Growing decision trees}

\begin{frame}[fragile]
    \frametitle{Supervised learning}

    \begin{itemize}
    \item Data comes as a finite learning set ${\cal L} = \texttt{(\X{X}, \y{y})}$ where
        \begin{itemize}
            \item \X{Input samples} are given as an array of shape \texttt{(n\_samples, n\_features)}\\
            \vspace{0.25cm}
            E.g., feature values for wine physicochemical properties:
\begin{verbatim}
# fixed acidity, volatile acidity, ...
X = [[  7.4    0.     ...   0.56   9.4    0.  ]
     [  7.8    0.     ...   0.68   9.8    0.  ]
                      ...
     [  7.8    0.04   ...   0.65   9.8    0.  ]]
\end{verbatim}
\vspace{0.25cm}

            \item \y{Output values} are given as an array of shape \texttt{(n\_samples,)}\\
            \vspace{0.25cm}
            E.g., wine taste preferences (from 0 to 10):
\begin{verbatim}
y = [5 5 5 ... 6 7 6]
\end{verbatim}
        \end{itemize}

    \vspace{0.25cm}

    \item The goal is to build an estimator $\model{\varphi_{\cal L}}: \X{{\cal X}} \mapsto \y{{\cal Y}}$ minimizing
    $$
    Err(\model{\varphi_{\cal L}}) = \mathbb{E}_{\X{X},\y{Y}}\{ L(\y{Y}, \model{\varphi_{\cal L}}\texttt{.predict(}\X{X}\texttt{)}) \}.
    $$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision trees \citep{breiman:1984}}
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-partition-d.pdf}
            \end{figure}
        \end{column}
        \begin{column}{0.7\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-simple.pdf}
            \end{figure}
        \end{column}
    \end{columns}

{\scriptsize
\begin{algorithmic}
\Function{BuildDecisionTree}{${\cal L}$}
    \State Create node $t$
    \If{the stopping criterion is met for $t$}
        \State Assign a model to $\widehat{y}_t$
    \Else
        \State Find the split on ${\cal L}$ that maximizes impurity decrease $$s^* = \argmax_{s} i(t) - p_L i(t^s_L) - p_R i(t^s_R)$$
        \State Partition ${\cal L}$ into ${\cal L}_{t_L} \cup {\cal L}_{t_R}$ according to $s^*$
        \State $t_L = \Call{BuildDecisionTree}{{\cal L}_{t_L}}$
        \State $t_R = \Call{BuildDecisionTree}{{\cal L}_{t_R}}$
    \EndIf
    \State \Return $t$
\EndFunction
\end{algorithmic}
}

\end{frame}

\begin{frame}
    \frametitle{Composability of decision trees}

    Decision trees can be used to solve several machine learning tasks by
    swapping the impurity and leaf model functions:

    \vspace{0.5cm}

    \begin{block}{0-1 loss (classification)}
    $\widehat{y}_t = \argmax_{c \in {\cal Y}} p(c|t)$, $i(t) = \text{entropy}(t)$ or $i(t) = \text{gini}(t)$
    \end{block}

    \begin{block}{Mean squared error (regression)}
    $\widehat{y}_t = \text{mean}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} (y - \widehat{y}_t)^2$
    \end{block}

    \begin{block}{Least absolute deviance (regression)}
    $\widehat{y}_t = \text{median}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} |y - \widehat{y}_t|$
    \end{block}

    \begin{block}{Density estimation}
    $\widehat{y}_t = {\cal N}(\mu_t, \Sigma_t)$, $i(t) = \text{differential entropy}(t)$
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{sklearn.tree}}

{\scriptsize
\begin{pythoncode}
# Fit a decision tree
from sklearn.tree import DecisionTreeRegressor
estimator = DecisionTreeRegressor(criterion="mse",   # Set i(t) function
                                  max_leaf_nodes=5)  # Tune model complexity
                                                     # with max_leaf_nodes,
                                                     # max_depth or
                                                     # min_samples_split
estimator.fit(X_train, y_train)

# Predict target values
y_pred = estimator.predict(X_test)

# MSE on test data
from sklearn.metrics import mean_squared_error
score = mean_squared_error(y_test, y_pred)
>>> 0.572049826453
\end{pythoncode}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Visualize and interpret}

{\footnotesize
\begin{pythoncode}
# Display tree
from sklearn.tree import export_graphviz
export_graphviz(estimator, out_file="tree.dot",
                feature_names=feature_names)
\end{pythoncode}
}

\begin{figure}
\includegraphics[width=0.8\textwidth]{./figures/wine-tree.png}
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses of decision trees}

  \begin{itemize}
    \item {\color{blue} Non-parametric} model, proved to be
          {\color{blue} consistent}.

    \vspace{0.25cm}

    \item Support {\color{blue} heterogeneous} data (continuous, ordered or
          categorical variables).

    \vspace{0.25cm}

    \item {\color{blue}Flexibility} in loss functions (but choice is
          {\color{red}limited}).

    \vspace{0.25cm}

    \item {\color{blue} Fast} to train, {\color{blue} fast} to predict.
        \begin{itemize}
            \item In the average case, complexity of training is $\Theta(pN\log^2 N)$.
        \end{itemize}

    \vspace{0.25cm}

    \item Easily {\color{blue} interpretable}.

    \vspace{0.25cm}

    \item {\color{blue} Low bias}, but usually {\color{red} high variance}\\
        \begin{itemize}
            \item Solution: Combine the predictions of several randomized
                  trees into a single model.
        \end{itemize}
  \end{itemize}
\end{frame}


% Forests and boosting ========================================================

\section{Random Forests}

\begin{frame}
    \frametitle{Random Forests \citep{breiman:2001,geurts:2006}}

    \begin{figure}
        \includegraphics[scale=0.5]{./figures/forest.pdf}
    \end{figure}

    Randomization

    \vspace{0.1cm}

    {\scriptsize
    \begin{tabular}{lll}

    \textbullet\hspace*{0.1cm} Bootstrap samples & \multirow{2}{*}{{\LARGE \}} {\color{blue} Random Forests}} & \\
    \textbullet\hspace*{0.1cm} Random selection of $K \leq p$ split variables && \multirow{2}{*}{{\LARGE \}} {\color{blue} Extra-Trees}} \\
    \textbullet\hspace*{0.1cm} Random selection of the threshold  & \\
    \end{tabular}}
\end{frame}

\begin{frame}{Bias and variance}
    \begin{figure}
        \includegraphics[scale=0.7]{./figures/bias-variance-darts.jpg}
    \end{figure}
\end{frame}

\begin{frame}{Bias-variance decomposition}

{\bf Theorem.}
For the squared error loss, the bias-variance decomposition of the expected
generalization error $\mathbb{E}_{\cal L} \{ Err( \psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x}))
\}$ at $X=\mathbf{x}$ of an ensemble of $M$ randomized models $\varphi_{{\cal L},\theta_m}$ is
\begin{equation*}
\mathbb{E}_{\cal L} \{ Err(\psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x})) \} = \text{noise}(\mathbf{x}) + \text{bias}^2(\mathbf{x}) + \text{var}(\mathbf{x}),
\end{equation*}
where
\begin{align*}
\text{noise}(\mathbf{x}) &= Err(\varphi_B(\mathbf{x})), \\
\text{bias}^2(\mathbf{x}) &= (\varphi_B(\mathbf{x}) - \mathbb{E}_{{\cal L},\theta} \{ \varphi_{{\cal L},\theta}(\mathbf{x}) \} )^2, \\
\text{var}(\mathbf{x}) &= \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x}).
\end{align*}

and where $\rho(\mathbf{x})$ is the Pearson correlation coefficient between
the predictions of two randomized trees built on the same learning set.

\end{frame}


\begin{frame}{Diagnosing the error of random forests \citep{louppe2014understanding}}

\begin{itemize}
\item Bias: {\color{blue} Identical} to the bias of a single randomized tree.
\item Variance: $\text{var}(\mathbf{x}) = \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x})$\\
As $M \to \infty$, {\color{red} $\text{var}(\mathbf{x}) \to \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x})$}
  \begin{itemize}
    \item The stronger the randomization, $\rho(\mathbf{x}) \to 0$, $\text{var}(\mathbf{x}) \to 0$.
    \item The weaker the randomization, $\rho(\mathbf{x}) \to 1$, $\text{var}(\mathbf{x}) \to \sigma^2_{{\cal L},\theta}(\mathbf{x})$
  \end{itemize}
\end{itemize}

\vspace{1cm}

{\bf Bias-variance trade-off.} Randomization increases bias but makes it
possible to reduce the variance of the corresponding ensemble model. The crux
of the problem is to {\color{red} find the right trade-off}.

\end{frame}


\begin{frame}[fragile]
\frametitle{Tuning randomization in \texttt{sklearn.ensemble}}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.cross_validation import ShuffleSplit
from sklearn.learning_curve import validation_curve

# Validation of max_features, controlling randomness in forests
param_name = "max_features"
param_range = range(1, X.shape[1]+1)

for Forest, color, label in [(RandomForestRegressor, "g", "RF"),
                             (ExtraTreesRegressor, "r", "ETs")]:
    _, test_scores = validation_curve(
        Forest(n_estimators=100, n_jobs=-1), X, y,
        cv=ShuffleSplit(n=len(X), n_iter=10, test_size=0.25),
        param_name=param_name, param_range=param_range,
        scoring="mean_squared_error")
    test_scores_mean = np.mean(-test_scores, axis=1)
    plt.plot(param_range, test_scores_mean, label=label, color=color)

plt.xlabel(param_name)
plt.xlim(1, max(param_range))
plt.ylabel("MSE")
plt.legend(loc="best")
plt.show()
\end{pythoncode}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Tuning randomization in \texttt{sklearn.ensemble}}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{./figures/randomness.png}
    \end{figure}

    \begin{center}
        Best-tradeoff: ExtraTrees, for \texttt{max\_features=6}.
    \end{center}
\end{frame}


\begin{frame}
  \frametitle{Strengths and weaknesses of forests}

  \begin{itemize}

    \item {\color{blue}Fine control} of bias and variance through averaging
          and randomization, resulting in {\color{blue} better performance}.

    \vspace{0.25cm}

    \item Moderately fast to train and to predict.
        \begin{itemize}
            \item $\Theta(MK\widetilde{N}\log^2 \widetilde{N})$ for RFs (where $\widetilde{N} = 0.632N$)
            \item $\Theta(MKN\log N)$ for ETs
        \end{itemize}

    \vspace{0.25cm}

    \item Embarrassingly {\color{blue} parallel} (use \texttt{n\_jobs}).

    \vspace{0.25cm}

    \item {\color{red} Less interpretable} than decision trees.
  \end{itemize}
\end{frame}


% Forests and boosting ========================================================

\section{Boosting}

\begin{frame}
  \frametitle{Gradient Boosted Regression Trees \citep{friedman:2001}}

  \begin{itemize}
  \item GBRT fits an additive model of the form
  $$\varphi(x) =  \sum_{m=1}^M \gamma_m h_m(x)$$

  \item The ensemble is built in a forward stagewise manner, where each
        regression tree $h_m$ is an approximate successive gradient step.
  \end{itemize}

    \begin{figure}
       \centering
       \includegraphics[width=\textwidth]{./figures/residual_fitting_2.pdf}
    \end{figure}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Careful tuning required}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.cross_validation import ShuffleSplit
from sklearn.grid_search import GridSearchCV

# Careful tuning is required to obtained good results
param_grid = {"learning_rate": [0.1, 0.01, 0.001],
              "subsample": [1.0, 0.9, 0.8],
              "max_depth": [3, 5, 7],
              "min_samples_leaf": [1, 3, 5]}

est = GradientBoostingRegressor(n_estimators=1000)
grid = GridSearchCV(est, param_grid,
                    cv=ShuffleSplit(n=len(X), n_iter=10, test_size=0.25),
                    scoring="mean_squared_error",
                    n_jobs=-1).fit(X, y)

gbrt = grid.best_estimator_
\end{pythoncode}
}

\begin{center}
See our PyData 2014 tutorial for further guidance
\url{https://github.com/pprett/pydata-gbrt-tutorial}
\end{center}

\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses of GBRT}

  \begin{itemize}
        \item {\color{blue} Flexible framework}, that can adapt to arbitrary
              loss functions.

        \vspace{0.25cm}

        \item Fine control of under/overfitting through {\color{blue}
              regularization} (e.g., learning rate, subsampling,
              tree structure, penalization term in the loss function, etc).

        \vspace{0.25cm}

        \item {\color{red} Careful tuning} required.

        \vspace{0.25cm}

        \item {\color{red} Slow} to train, {\color{blue} fast} to predict.
  \end{itemize}
\end{frame}

% Framework (loss function, regularization, etc)


% Reading tree leaves =========================================================

\section{Reading tree leaves}

\begin{frame}[fragile]
  \frametitle{Variable importances}

{\scriptsize
\begin{pythoncode}
importances = pd.DataFrame()

# Variable importances with Random Forest, default parameters
est = RandomForestRegressor(n_estimators=10000, n_jobs=-1).fit(X, y)
importances["RF"] = pd.Series(est.feature_importances_,
                              index=feature_names)

# Variable importances with Totally Randomized Trees
est = ExtraTreesRegressor(max_features=1, max_depth=3,
                          n_estimators=10000, n_jobs=-1).fit(X, y)
importances["TRTs"] = pd.Series(est.feature_importances_,
                                index=feature_names)

# Variable importances with GBRT
importances["GBRT"] = pd.Series(gbrt.feature_importances_,
                                index=feature_names)

importances.plot(kind="barh")
\end{pythoncode}
}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Variable importances}

\begin{figure}
\includegraphics[width=0.95\textwidth]{./figures/importances.png}
\end{figure}

\begin{center}
Importances are measured only through the eyes of the model.
{\color{red}They may not tell the entire nor the same story!} \citep{louppe:2013}
\end{center}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Partial dependence plots}

Relation between the response $Y$ and a subset of features, marginalized over all other features.

\vspace{0.25cm}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble.partial_dependence import plot_partial_dependence
plot_partial_dependence(gbrt, X,
                        features=[1, 10], feature_names=feature_names)
\end{pythoncode}
}

    \begin{figure}
       \centering
       \includegraphics[width=0.9\textwidth]{./figures/dependence.png}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Embedding}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble import RandomTreesEmbedding
from sklearn.decomposition import TruncatedSVD

# Project wines through a forest of totally randomized trees
# and use the leafs the samples end into as a high-dimensional representation
hasher = RandomTreesEmbedding(n_estimators=1000)
X_transformed = hasher.fit_transform(X)

# Plot wines on a plane using the 2 principal components
svd = TruncatedSVD(n_components=2)
coords = svd.fit_transform(X_transformed)

n_values = 10 + 1  # Wine preferences are from 0 to 10
cm = plt.get_cmap("hsv")
colors =  (cm(1. * i / n_values) for i in range(n_values))

for k, c in zip(range(n_values), colors):
    plt.plot(coords[y == k, 0], coords[y == k, 1], '.', label=k, color=c)

plt.legend()
plt.show()
\end{pythoncode}
}

\end{frame}


\begin{frame}[fragile]
  \frametitle{Embedding}

    \begin{figure}
       \centering
       \includegraphics[width=0.95\textwidth]{./figures/embedding.png}
    \end{figure}

    \begin{center}
        Can you guess what these 2 clusters correspond to?
    \end{center}

\end{frame}



% Summary =================================================================

\section{Summary}

\begin{frame}
  \frametitle{Summary}

  \begin{itemize}
        \item Tree-based methods offer a {\color{blue}flexible and efficient non-parametric} framework
              for classification and regression.

        \vspace{0.25cm}

        \item Applicable to a wide variety of problems, with a {\color{blue}fine control}
              over the model that is learned.

        \vspace{0.25cm}

        \item Assume a good feature representation -- i.e., tree-based methods are often {\color{red}not that good on very
              raw input data}, like pixels, speech signals, etc.

        \vspace{0.25cm}

        \item {\color{blue}Insights on the problem} under study (variable importances,
              dependence plots, embedding, ...).

        \vspace{0.25cm}

        \item {\color{blue}Efficient implementation} in Scikit-Learn.
  \end{itemize}
\end{frame}

\begin{frame}[plain,noframenumbering]
    \begin{columns}
    \begin{column}{0.5\textwidth}

    Join us on \url{https://github.com/scikit-learn/scikit-learn}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
      \includegraphics[width=\textwidth]{./figures/we_want_you.png}
      \end{figure}
    \end{column}
    \end{columns}
\end{frame}

\begin{frame}[plain,noframenumbering]
    \frametitle{References}
    {\footnotesize
    \bibliographystyle{apalike}
    \bibliography{biblio}}
\end{frame}

\end{document}
