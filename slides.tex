\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{etex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{algpseudocode}

% Code snippets
\usepackage{minted}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
\newminted{python}{bgcolor=bgcolor}

% Checked marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Colors
\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

% Beamer layout
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

% Front slide
\title{{\bf Tree models with Scikit-Learn}\\
Great learners with little assumptions}
\author{
Material: \url{https://github.com/glouppe/talk-pydata2015}\\
\vspace{1cm}
Gilles Louppe (\href{https://twitter.com/glouppe}{@glouppe})\\
{\it CERN}
}
\date{PyData, April 3, 2015}

% Argmax
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{frame}
\titlepage
\end{frame}


% Random things to talk about:
% - class_weights
% - oob samples
% - n_jobs


% Outline =====================================================================

\begin{frame}
  \frametitle{Outline}
  %\tableofcontents
  \setbeamertemplate{enumerate items}[circle]
  \begin{enumerate}
  \item Motivation

  \vspace{0.5cm}

  \item Growing decision trees

  \vspace{0.5cm}

  \item Random forests

    \vspace{0.5cm}

  \item Boosting

  \vspace{0.5cm}

  \item Reading tree leaves

  \vspace{0.5cm}

  \item Summary
  \end{enumerate}
\end{frame}


% Motivation ==================================================================

\section{Motivation}

\begin{frame}
    \frametitle{Motivation}
    \begin{figure}
    \vspace{-0.5cm}
    \includegraphics[scale=0.4]{./figures/motivation.png}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Running example}

    \begin{columns}
    \begin{column}{0.5\textwidth}

    \begin{center}
    From {\bf \X{physicochemical properties}} (alcohol, acidity, sulphates, ...),

    \vspace{1cm}
    learn a {\bf \model{model}}
    \vspace{1cm}

    to predict {\bf \y{wine taste preferences}}.

    \end{center}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
      \vspace{-0.5cm}
      \includegraphics[scale=0.6]{./figures/wine.jpg}
      \end{figure}
    \end{column}
    \end{columns}
\end{frame}


% Growing decision trees ======================================================

\AtBeginSection[]
{
\begin{frame}
  \frametitle{Outline}
  \tableofcontents[currentsection]
  % Die Option [pausesections]
\end{frame}
}

\section{Growing decision trees}

\begin{frame}[fragile]
    \frametitle{Supervised learning}

    \begin{itemize}
    \item Data comes as a finite learning set ${\cal L} = \texttt{(\X{X}, \y{y})}$ where
        \begin{itemize}
            \item \X{Input samples} are given as an array of shape \texttt{(n\_samples, n\_features)}\\
            \vspace{0.25cm}
            E.g., feature values for wine physicochemical properties:
\begin{verbatim}
# fixed acidity, volatile acidity, ...
X = [[  7.4    0.     ...   0.56   9.4    0.  ]
     [  7.8    0.     ...   0.68   9.8    0.  ]
                      ...
     [  7.8    0.04   ...   0.65   9.8    0.  ]]
\end{verbatim}
\vspace{0.25cm}

            \item \y{Output values} are given as an array of shape \texttt{(n\_samples,)}\\
            \vspace{0.25cm}
            E.g., wine taste preferences (from 0 to 10):
\begin{verbatim}
y = [5 5 5 ... 6 7 6]
\end{verbatim}
        \end{itemize}

    \vspace{0.25cm}

    \item The goal is to build an estimator $\model{\varphi_{\cal L}}: \X{{\cal X}} \mapsto \y{{\cal Y}}$ minimizing
    $$
    Err(\model{\varphi_{\cal L}}) = \mathbb{E}_{\X{X},\y{Y}}\{ L(\y{Y}, \model{\varphi_{\cal L}}\texttt{.predict(}\X{X}\texttt{)}) \}.
    $$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision trees [L. Breiman, 1984]}
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-partition-d.pdf}
            \end{figure}
        \end{column}
        \begin{column}{0.7\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-simple.pdf}
            \end{figure}
        \end{column}
    \end{columns}

{\scriptsize
\begin{algorithmic}
\Function{BuildDecisionTree}{${\cal L}$}
    \State Create node $t$
    \If{the stopping criterion is met for $t$}
        \State Assign a model to $\widehat{y}_t$
    \Else
        \State Find the split on ${\cal L}$ that maximizes impurity decrease $$s^* = \argmax_{s} i(t) - p_L i(t^s_L) - p_R i(t^s_R)$$
        \State Partition ${\cal L}$ into ${\cal L}_{t_L} \cup {\cal L}_{t_R}$ according to $s^*$
        \State $t_L = \Call{BuildDecisionTree}{{\cal L}_{t_L}}$
        \State $t_R = \Call{BuildDecisionTree}{{\cal L}_{t_R}}$
    \EndIf
    \State \Return $t$
\EndFunction
\end{algorithmic}
}

\end{frame}

\begin{frame}
    \frametitle{Composability of decision trees}

    Decision trees can be used to solve several machine learning tasks by simply swapping the impurity and leaf model functions.

    \vspace{0.5cm}

    \begin{block}{0-1 loss (classification)}
    $\widehat{y}_t = \argmax_{c \in {\cal Y}} p(c|t)$, $i(t) = \text{entropy}(t)$ or $i(t) = \text{gini}(t)$
    \end{block}

    \begin{block}{Mean squared error (regression)}
    $\widehat{y}_t = \text{mean}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} (y - \widehat{y}_t)^2$
    \end{block}

    \begin{block}{Least absolute deviance (regression)}
    $\widehat{y}_t = \text{median}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} |y - \widehat{y}_t|$
    \end{block}

    \begin{block}{Density estimation}
    $\widehat{y}_t = {\cal N}(\mu_t, \Sigma_t)$, $i(t) = \text{differential entropy}(t)$
    \end{block}
\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{sklearn.tree}}

{\footnotesize
\begin{pythoncode}
# Fit a decision tree
from sklearn.tree import DecisionTreeRegressor
estimator = DecisionTreeRegressor(max_leaf_nodes=5)
estimator.fit(X_train, y_train)
>>> DecisionTreeRegressor(criterion='mse', ...)

# Predict target values
y_pred = estimator.predict(X_test)

# R^2 score on test data
score = estimator.score(X_test, y_test)
>>> 0.230147545515
\end{pythoncode}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Visualize and interpret}

{\footnotesize
\begin{pythoncode}
# Display tree
from sklearn.tree import export_graphviz
export_graphviz(estimator, out_file="tree.dot",
                feature_names=feature_names)
\end{pythoncode}
}

\begin{figure}
\includegraphics[width=0.8\textwidth]{./figures/wine-tree.png}
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses}

  \begin{itemize}
    \item {\color{blue} Non-parametric} model
    \item {\color{blue} Fast} to train ($\Theta(pN\log^2 N)$)
    \item Easily {\color{blue} interpretable}
    \item {\color{blue} Low bias}, but usually {\color{red} high variance}
  \end{itemize}
\end{frame}


% Forests and boosting ========================================================

\section{Random Forests}

\begin{frame}
  \frametitle{Bias-variance decomposition}
\end{frame}

\begin{frame}
  \frametitle{Random Forests}
\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses}
\end{frame}


% Forests and boosting ========================================================

\section{Boosting}

\begin{frame}
  \frametitle{Gradient Boosted Regression Trees}
\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses}
\end{frame}

% Framework (loss function, regularization, etc)


% Reading tree leaves =========================================================

\section{Reading tree leaves}

\begin{frame}
  \frametitle{Reading tree leaves}
  % Visualizing trees
\end{frame}

\begin{frame}
  \frametitle{Variable importances}
\end{frame}

\begin{frame}
  \frametitle{Partial dependence plots}
\end{frame}

\begin{frame}
  \frametitle{Outlier detection}
\end{frame}

\begin{frame}
  \frametitle{Embedding}
\end{frame}



% Summary =================================================================

% strong points and weaknesses
% going further


\section{Summary}

\begin{frame}
  \frametitle{Summary}
\end{frame}

\begin{frame}
\begin{center}
{\Huge  Questions?}
\end{center}
\end{frame}

\end{document}
